---
title: "Test By R markdown"
description: |
  Predict quality of wine.
author:
  - name: Nguyen Duc Thang 
    url: https://example.com/norajones
date: "`r Sys.Date()`"
output: distill::distill_article
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
## Wine Dataset 
**Import required packages**
```{python,eval=FALSE, echo=TRUE}
import pandas as pd
import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, classification_report,accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder,StandardScaler
import seaborn as sns
from sklearn import neighbors

wine_dataset = pd.read_csv('winequality-red.csv')
wine_dataset_ = pd.read_csv('winequality-red.csv')
```
**Our dataset**
```{r}
library(reticulate)
library(rmarkdown)
source_python('test.py')
a <- wine_dataset_
paged_table(a)
```
**Affect of features to wine quality**
```{r,fig.align="center"}
library(ggplot2)
ggplot(a[,c("fixed acidity","quality")], aes(x=`quality`,y=`fixed acidity`,fill=quality)) + geom_bar(stat="summary") + scale_y_continuous(breaks=c(0,2,4,6,8,10)) 
```
*Fixed acidity does not give any specification to quality*
```{r, fig.align="center"}
ggplot(a[,c("volatile acidity","quality")], aes(x=`quality`,y=`volatile acidity`,fill=quality)) + geom_bar(stat="summary") 
```
*We see that quality tends to decrease as volatile acidity increases*
```{r, fig.align="center"}
ggplot(a[,c("citric acid","quality")], aes(x=`quality`,y=`citric acid`,fill=quality)) + geom_bar(stat="summary")
```
*quality tends to increase when citric acid increase* 
```{r, fig.align="center"}
ggplot(a[,c("residual sugar","quality")], aes(x=`quality`,y=`residual sugar`,fill=quality)) + geom_bar(stat="summary")
```
```{r, fig.align="center"}
ggplot(a[,c("chlorides","quality")], aes(x=`quality`,y=`chlorides`,fill=quality)) + geom_bar(stat="summary")
```
*Quality increases when chlorides increases*
```{r, fig.align="center"}
ggplot(a[,c("free sulfur dioxide","quality")], aes(x=`quality`,y=`free sulfur dioxide`,fill=quality)) + geom_bar(stat="summary")
```
```{r, fig.align="center"}
ggplot(a[,c("total sulfur dioxide","quality")], aes(x=`quality`,y=`total sulfur dioxide`,fill=quality)) + geom_bar(stat="summary")
```
```{r, fig.align="center"}
ggplot(a[,c("sulphates","quality")], aes(x=`quality`,y=`sulphates`,fill=quality)) + geom_bar(stat="summary")
```
```{r, fig.align="center"}
ggplot(a[,c("alcohol","quality")], aes(x=`quality`,y=`alcohol`,fill=quality)) + geom_bar(stat="summary")
```
## Predict wine quality use Machine Learning

- *Preprocessing Data*

```{python,eval=FALSE, echo=TRUE}
bins = [2,6.5,8]
group_names = ['bad','good']
wine_dataset['quality'] = pd.cut(wine_dataset['quality'],bins=bins,labels=group_names)
label_quality = LabelEncoder()
wine_dataset['quality'] = label_quality.fit_transform(wine_dataset['quality'])

X = wine_dataset.drop('quality', axis = 1)
Y = wine_dataset['quality']

X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=42)
sc = StandardScaler()

X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)
```


- *K nearest neighbors*

I choose K = 10 and weights = 'distance'

```{python,eval=FALSE, echo=TRUE}
def KnearestNeighborsClassifier():
    knn = neighbors.KNeighborsClassifier(n_neighbors=10,p=2, weights='distance')
    knn.fit(X_train,Y_train)
    pred_knn = knn.predict(X_test)
    return ["Accuracy score for 10NN: "  + str(100*accuracy_score(Y_test,pred_knn)) +"%",knn]
```

```{r}
KNN <- KnearestNeighborsClassifier()
cat(unlist(KNN[1]))
```

- *Support Vector Machine*

```{python,eval=FALSE, echo=TRUE}
def SVMClassifier():
    svc = SVC()
    svc.fit(X_train,Y_train)
    pred_svc = svc.predict(X_test)
    return ["Accuracy score for SVM: "  + str(100*accuracy_score(Y_test,pred_svc)) +"%",svc]
```

```{r}
svm_ <- SVMClassifier()
cat(unlist(svm_[1]))

```

- *Random Forest*

```{python,eval=FALSE,echo=TRUE}
def randomForestClassifier():
    rfc = RandomForestClassifier(n_estimators=200)
    rfc.fit(X_train,Y_train)
    pred_rfc = rfc.predict(X_test)
    return ["Accuracy score for random forest: "  + str(100*accuracy_score(Y_test,pred_rfc)) +"%",rfc]
```

```{r}
rfc <- randomForestClassifier()
cat(unlist(rfc[1]))
```
## Using GridSearch to find best parameters

- *Grid search for SVM*

```{python,eval=FALSE,echo=TRUE}
  def gridSearchForSVM():
    param = {
        'C': [0.1,0.8,0.9,1,1.1,1.2,1.3,1.4],
        'kernel':['linear', 'rbf'],
        'gamma' :[0.1,0.8,0.9,1,1.1,1.2,1.3,1.4]
    }
    grid_svc = GridSearchCV(SVMClassifier()[1], param_grid=param, scoring='accuracy', cv=10)
    grid_svc.fit(X_train,Y_train)
    para = grid_svc.best_params_
    svc2 = SVC(C = para['C'], gamma =  para['gamma'], kernel= para['kernel'])
    svc2.fit(X_train, Y_train)
    pred_svc2 = svc2.predict(X_test)
    return ["Accuracy score for SVM : "  + str(100*accuracy_score(Y_test,pred_svc2)) +"%",para]
```


```{r}
svm_grid <- gridSearchForSVM()

```
* best parameter for svm:

```{r}
paged_table(data.frame(svm_grid[2]))
```
* accuracy:

```{r}
cat(unlist(svm_grid[1]))
```

- *Grid Search for KNN*

```{python,eval=FALSE,echo=TRUE}
def gridSearchForKNN():
    grid_params = {
        'n_neighbors':[8,10,15,20],
        'weights': ['uniform','distance'],
        'metric': ['euclidean','manhattan']

    }
    gs  = GridSearchCV(
        KnearestNeighborsClassifier()[1],
        grid_params,
        verbose=1,
        cv = 3,
        n_jobs=1
    )
    gs_res = gs.fit(X_train,Y_train)
    para = gs_res.best_params_
    knn2 = neighbors.KNeighborsClassifier(n_neighbors=para['n_neighbors'],weights=para['weights'])
    knn2.fit(X_train,Y_train)
    pred_knn2 = knn2.predict(X_test)
    return ["Accuracy score for KNN : "  + str(100*accuracy_score(Y_test,pred_knn2)) +"%",para]
```

```{r}
knn_grid <- gridSearchForKNN()
cat(unlist(knn_grid[1]))
```
* best parameter for knn:

```{r}
df <- data.frame(knn_grid[2])
paged_table(df)
```
* accuracy:

```{r}
cat(unlist(knn_grid[1]))
```

- *Grid search for Random Forest*

```{python,eval=FALSE,echo=TRUE}
def gridSearchForRandomForest():
    param_grid = { 
    'n_estimators': [200, 500],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth' : [4,5,6,7,8],
    'criterion' :['gini', 'entropy']
    }
    CV_rfc = GridSearchCV(estimator=randomForestClassifier()[1], param_grid=param_grid, cv= 5)
    CV_rfc.fit(X_train, Y_train)
    para = CV_rfc.best_params_
    rfc1 = RandomForestClassifier(random_state=42, max_features=para['max_features'], n_estimators= para['n_estimators'], max_depth=para['max_depth'], criterion=para['criterion'])
    rfc1.fit(X_train,Y_train)
    pred_rfc1=rfc1.predict(x_test)
    return ["Accuracy score for RFC : "  + str(100*accuracy_score(Y_test,pred_rfc1)) +"%",para]
```
```{r,}
rfc_grid <- gridSearchForRandomForest()

```
* best parameter for RFC: 

```{r}
paged_table(data.frame(rfc_grid[2]))
```
* accuracy:

```{r}
cat(unlist(rfc_grid[1]))
```

